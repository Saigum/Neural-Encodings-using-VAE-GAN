{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561},{"sourceId":10022683,"sourceType":"datasetVersion","datasetId":6171892},{"sourceId":209249059,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/moshesipper/vae-torch-celeba.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:12.400265Z","iopub.execute_input":"2024-12-02T12:07:12.401032Z","iopub.status.idle":"2024-12-02T12:07:14.984269Z","shell.execute_reply.started":"2024-12-02T12:07:12.400997Z","shell.execute_reply":"2024-12-02T12:07:14.982967Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'vae-torch-celeba'...\nremote: Enumerating objects: 50, done.\u001b[K\nremote: Counting objects: 100% (36/36), done.\u001b[K\nremote: Compressing objects: 100% (24/24), done.\u001b[K\nremote: Total 50 (delta 22), reused 26 (delta 12), pack-reused 14 (from 1)\u001b[K\nReceiving objects: 100% (50/50), 28.72 MiB | 46.10 MiB/s, done.\nResolving deltas: 100% (23/23), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%cd vae-torch-celeba","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:14.987192Z","iopub.execute_input":"2024-12-02T12:07:14.987653Z","iopub.status.idle":"2024-12-02T12:07:14.995330Z","shell.execute_reply.started":"2024-12-02T12:07:14.987606Z","shell.execute_reply":"2024-12-02T12:07:14.994239Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/vae-torch-celeba\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport os\nimport numpy as np\nfrom torch.utils.data import Subset\nfrom torchvision.utils import save_image\n\n# Constants from your VAE model\nIMAGE_SIZE = 150\nLATENT_DIM = 128\nimage_dim = 3 * IMAGE_SIZE * IMAGE_SIZE\n\n# Set seed for reproducibility\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    \nset_seed(42)\n\nclass CelebADataset(Dataset):\n    def __init__(self, root_dir, partition_file, attr_file, transform=None):\n        self.root_dir = root_dir\n        self.img_dir = os.path.join(root_dir, 'img_align_celeba', 'img_align_celeba')\n        self.partition_df = pd.read_csv(partition_file)\n        self.attr_df = pd.read_csv(attr_file)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.partition_df)\n    \n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir, self.partition_df.iloc[idx, 0])\n        image = Image.open(img_name)\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        # Flatten the image to match VAE requirements\n        return image.view(-1)\n\ndef get_celeba_dataloaders(root_dir, batch_size=64, subset_fraction=0.1):\n    # Define transforms according to VAE specifications\n    transform = transforms.Compose([\n        transforms.Resize(IMAGE_SIZE, antialias=True),\n        transforms.CenterCrop(IMAGE_SIZE),\n        transforms.ToTensor()\n    ])\n    \n    # Create dataset\n    full_dataset = CelebADataset(\n        root_dir=root_dir,\n        partition_file=os.path.join(root_dir, 'list_eval_partition.csv'),\n        attr_file=os.path.join(root_dir, 'list_attr_celeba.csv'),\n        transform=transform\n    )\n    \n    # Get indices for train/val/test\n    partition_df = pd.read_csv(os.path.join(root_dir, 'list_eval_partition.csv'))\n    train_indices = partition_df[partition_df['partition'] == 0].index.tolist()\n    val_indices = partition_df[partition_df['partition'] == 1].index.tolist()\n    test_indices = partition_df[partition_df['partition'] == 2].index.tolist()\n    \n    # Take subset if requested\n    num_train_samples = int(len(train_indices) * subset_fraction)\n    train_indices = train_indices[:num_train_samples]\n    \n    # Create subset datasets\n    train_dataset = Subset(full_dataset, train_indices)\n    val_dataset = Subset(full_dataset, val_indices)\n    test_dataset = Subset(full_dataset, test_indices)\n    \n    print(f\"Number of training samples: {len(train_dataset)}\")\n    print(f\"Number of validation samples: {len(val_dataset)}\")\n    print(f\"Number of test samples: {len(test_dataset)}\")\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader, test_loader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:14.997233Z","iopub.execute_input":"2024-12-02T12:07:14.997557Z","iopub.status.idle":"2024-12-02T12:07:19.309940Z","shell.execute_reply.started":"2024-12-02T12:07:14.997531Z","shell.execute_reply":"2024-12-02T12:07:19.308698Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"pd.set_option('future.no_silent_downcasting', True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:19.311703Z","iopub.execute_input":"2024-12-02T12:07:19.312100Z","iopub.status.idle":"2024-12-02T12:07:19.316477Z","shell.execute_reply.started":"2024-12-02T12:07:19.312071Z","shell.execute_reply":"2024-12-02T12:07:19.315293Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def generate_reconstructions(model, test_loader, device, num_images=8):\n    model.eval()\n    model = model.to(device)  # Move model to GPU\n    \n    with torch.no_grad():\n        # Get one batch\n        for imgs in test_loader:\n            pics = imgs.to(device)  # Move images to GPU\n            # Reshape for the model\n            pics = pics.view(-1, 3, IMAGE_SIZE, IMAGE_SIZE)\n            break\n            \n        # Keep first num_images\n        pics = pics[:num_images]\n        orig = torch.clone(pics)\n        \n        # Generate reconstructions\n        all_pics = [orig]\n        current_pics = orig\n        for _ in range(7):\n            recon, _, _ = model(current_pics)\n            recon = recon.view(-1, 3, IMAGE_SIZE, IMAGE_SIZE)\n            all_pics.append(recon)\n            current_pics = recon\n            \n        # Stack all images\n        final_pics = torch.cat(all_pics, dim=0)\n        \n        # Move back to CPU for saving\n        final_pics = final_pics.cpu()\n        \n        # Save the grid\n        save_image(\n            final_pics, \n            'reconstructions2.jpg', \n            nrow=num_images, \n            normalize=True\n        )\n\n# Usage\nroot_dir = '/kaggle/input/celeba-dataset'\ntrain_loader, val_loader, test_loader = get_celeba_dataloaders(\n    root_dir=root_dir,\n    batch_size=64,\n    subset_fraction=1.0\n)\n\n# Set up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load model\nMODEL_FILE = 'vae_model_20.pth'\nmodel = torch.load(MODEL_FILE, map_location=device, weights_only=False)\n\n# Generate reconstructions\ngenerate_reconstructions(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:19.317483Z","iopub.execute_input":"2024-12-02T12:07:19.317758Z","iopub.status.idle":"2024-12-02T12:07:22.992914Z","shell.execute_reply.started":"2024-12-02T12:07:19.317732Z","shell.execute_reply":"2024-12-02T12:07:22.991775Z"}},"outputs":[{"name":"stdout","text":"Number of training samples: 162770\nNumber of validation samples: 19867\nNumber of test samples: 19962\nUsing device: cuda\n\nCELEB_PATH ./data/ IMAGE_SIZE 150 LATENT_DIM 128 image_dim 67500\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:22.994290Z","iopub.execute_input":"2024-12-02T12:07:22.995041Z","iopub.status.idle":"2024-12-02T12:07:24.058470Z","shell.execute_reply.started":"2024-12-02T12:07:22.995010Z","shell.execute_reply":"2024-12-02T12:07:24.057618Z"}},"outputs":[{"name":"stdout","text":"LICENSE    __pycache__\treconstructions2.jpg  utils.py\tvae_model_20.pth\nREADME.md  genpics.py\ttrainvae.py\t      vae.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!ls\n%pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:24.060197Z","iopub.execute_input":"2024-12-02T12:07:24.061122Z","iopub.status.idle":"2024-12-02T12:07:25.120555Z","shell.execute_reply.started":"2024-12-02T12:07:24.061079Z","shell.execute_reply":"2024-12-02T12:07:25.119410Z"}},"outputs":[{"name":"stdout","text":"LICENSE    __pycache__\treconstructions2.jpg  utils.py\tvae_model_20.pth\nREADME.md  genpics.py\ttrainvae.py\t      vae.py\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/vae-torch-celeba'"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"attr = pd.read_csv(\"/kaggle/input/celeba-dataset/list_attr_celeba.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:25.122361Z","iopub.execute_input":"2024-12-02T12:07:25.123363Z","iopub.status.idle":"2024-12-02T12:07:25.785099Z","shell.execute_reply.started":"2024-12-02T12:07:25.123305Z","shell.execute_reply":"2024-12-02T12:07:25.784352Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import torch as th\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch as th\nimport nibabel as nib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom torchvision import transforms\nfrom PIL import Image\nimport gc\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm \n\nclass FmriConvNet(nn.Module):\n    def __init__(self):\n        super(FmriConvNet, self).__init__()\n\n        # Convolutional Block 1\n        self.conv1 = nn.Conv3d(in_channels=1, out_channels=8, kernel_size=3, stride=2, padding=1)\n        self.relu1 = nn.ReLU()\n\n        # Convolutional Block 2\n        self.conv2 = nn.Conv3d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1)\n        self.relu2 = nn.ReLU()\n\n        # Convolutional Block 3\n        self.conv3 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n        self.relu3 = nn.ReLU()\n\n        # Convolutional Block 4\n        self.conv4 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1)\n        self.relu4 = nn.ReLU()\n\n        # Convolutional Block 5\n        self.conv5 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(5,5,3), stride=1, padding=0)\n        self.relu5 = nn.ReLU()\n\n        # Flatten and Fully Connected Layers\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(128 * 1 * 1 * 1, 1024)  # Reduced input size\n        self.relu_fc1 = nn.ReLU()\n        self.output = nn.Linear(1024, 1024)\n\n        self.mu = nn.Linear(1024, 128)\n        self.logvar = nn.Linear(1024, 128)\n\n    def forward(self, x):\n        x = self.relu1(self.conv1(x))  # Output shape: (batch_size, 8, 40, 40, 21)\n        x = self.relu2(self.conv2(x))  # Output shape: (batch_size, 16, 20, 20, 11)\n        x = self.relu3(self.conv3(x))  # Output shape: (batch_size, 32, 10, 10, 6)\n        x = self.relu4(self.conv4(x))  # Output shape: (batch_size, 64, 5, 5, 3)\n\n        x = self.relu5(self.conv5(x))  # Output shape: (batch_size, 128, 1, 1, 1)\n\n        x = self.flatten(x)            # Output shape: (batch_size, 128)\n        x = self.relu_fc1(self.fc1(x)) # Output shape: (batch_size, 1024)\n        x = self.output(x)             # Output shape: (batch_size, 1024)\n\n        mu = self.mu(x)\n        logvar = self.logvar(x)\n        return mu, logvar\nbrain2latent = FmriConvNet().to(device)\n# br2l_path = \"/kaggle/input/brain2latent/pytorch/default/1/brain2latent_final3.pth\"\n# brain2latent.load_state_dict(th.load(br2l_path)[\"model_state_dict\"])\n\n# with th.no_grad():\n#     output = brain2latent(th.randn(5, 1, 80, 80, 41).float().to(device))\n#     print(output[0].shape)\n\ntransform = transforms.Compose([\n        transforms.Resize(IMAGE_SIZE, antialias=True),\n        transforms.CenterCrop(IMAGE_SIZE),\n        transforms.ToTensor()\n    ])\ndef load_and_transform_image(image_path):\n    \"\"\"\n    Loads an image, applies transformations, and returns the transformed image tensor\n    in the shape (1, 3, 128, 128).\n    \"\"\"\n    image = Image.open(image_path)\n    transformed_image = transform(image)\n    return transformed_image\nheader = \"/kaggle/input/ds-sub2-download/ds001761-download/\"\nall_events=[]\nfor i in range(1,3):\n    for j in range(1,9):\n        for k in range(1,9):\n            stub = header+f\"sub-0{i}/ses-0{j}/func/sub-0{i}_ses-0{j}_task-faces_run-0{k}\"\n            event_path = stub +\"_events.tsv\"\n            fmri_path = stub +\"_bold.nii.gz\"\n            event_obj = pd.read_csv(event_path,delimiter='\\t')\n            event_obj[\"stim_file\"]= \"/kaggle/input/ds-sub2-download/ds001761-download/stimuli/\" + event_obj[\"stim_file\"]\n            event_obj[\"fmri_map\"] = (event_obj[\"onset\"]/2).round()\n            event_obj[\"subject_number\"] = i\n            event_obj[\"session_number\"] = j\n            event_obj[\"run_number\"] = k\n            all_events.append(event_obj)\n            \nevent_set = pd.concat(all_events)\nevent_set = event_set[event_set[\"stim_file\"] != \"/kaggle/input/ds-sub2-download/ds001761-download/stimuli/placeholders/fixation.png\"]\nimg2name = \"/kaggle/input/images2celeba-txt/ImageNames2Celeba.txt\"\nimg_df = pd.read_csv(img2name,delimiter='\\t',names=[\"stim_file\",\"image_id\"])\nimg_df[\"stim_file\"] = \"/kaggle/input/ds-sub2-download/ds001761-download/stimuli/\" + img_df[\"stim_file\"]\nevent_set = pd.merge(event_set,img_df,on=\"stim_file\",how=\"inner\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:25.788741Z","iopub.execute_input":"2024-12-02T12:07:25.789068Z","iopub.status.idle":"2024-12-02T12:07:28.199284Z","shell.execute_reply.started":"2024-12-02T12:07:25.789041Z","shell.execute_reply":"2024-12-02T12:07:28.198502Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"event_set = pd.merge(event_set,attr,on=\"image_id\",how=\"inner\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:28.200571Z","iopub.execute_input":"2024-12-02T12:07:28.200981Z","iopub.status.idle":"2024-12-02T12:07:28.279328Z","shell.execute_reply.started":"2024-12-02T12:07:28.200938Z","shell.execute_reply":"2024-12-02T12:07:28.278494Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"attribute_list =  ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n       'Wearing_Necklace', 'Wearing_Necktie', 'Young']\nprint(len(attribute_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:28.280499Z","iopub.execute_input":"2024-12-02T12:07:28.280798Z","iopub.status.idle":"2024-12-02T12:07:28.286922Z","shell.execute_reply.started":"2024-12-02T12:07:28.280770Z","shell.execute_reply":"2024-12-02T12:07:28.286064Z"}},"outputs":[{"name":"stdout","text":"40\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nfrom collections import defaultdict\n\ndef get_class(attr_vals):\n    hash=0\n    pow=1\n    for val in attr_vals:\n        hash += val*pow\n        pow*=2\n    return hash\n\n\nimportant_attr = [\"Male\"]\nevent_set[\"class\"] = 0\nclass_batches = defaultdict(list)\nfor i, row in event_set.iterrows():\n    attributes = row[important_attr].fillna(0).astype(int).values\n    hashval = get_class(attributes)\n    event_set.at[i, \"class\"] = hashval\n    class_batches[hashval].append(i)\n #print(event_set.iloc[i][\"class\"],hashval)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:28.288281Z","iopub.execute_input":"2024-12-02T12:07:28.288981Z","iopub.status.idle":"2024-12-02T12:07:34.929823Z","shell.execute_reply.started":"2024-12-02T12:07:28.288936Z","shell.execute_reply":"2024-12-02T12:07:34.929054Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class_counts = event_set[\"class\"].value_counts()\nsubset = class_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:34.930796Z","iopub.execute_input":"2024-12-02T12:07:34.931046Z","iopub.status.idle":"2024-12-02T12:07:34.940511Z","shell.execute_reply.started":"2024-12-02T12:07:34.931021Z","shell.execute_reply":"2024-12-02T12:07:34.939540Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(min(subset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:34.941595Z","iopub.execute_input":"2024-12-02T12:07:34.941914Z","iopub.status.idle":"2024-12-02T12:07:34.951966Z","shell.execute_reply.started":"2024-12-02T12:07:34.941888Z","shell.execute_reply":"2024-12-02T12:07:34.951059Z"}},"outputs":[{"name":"stdout","text":"6119\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# from torch.data.utils import Sampler\n# ## gonna make a dataset that returns the minibatch curated according to contrastive loss\n\n# class contr_fmri(Sampler):\n#     def __init__(self,classcounts,batch_indices,batch_size):\n#         #self.event_set = event_set\n#         #self.transform=transform\n#         #self.attribute_list = attribute_list\n#         self.classcounts = classcounts\n#         self.class_hashes = classcounts.index.tolist()\n#         self.batch_indices=batch_indices\n#     def __len__(self):\n#         return(min(self.classcounts))\n#     def __iter__(self):\n#         examples = defaultdict(list)\n#         for(hash in self.class_hashes):\n#             ## append indices of pandas dataframe\n#             examples[hash].append(self.batch_indices[hash][idx:idx+batch_size])\n            \n            \n        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:34.952951Z","iopub.execute_input":"2024-12-02T12:07:34.953276Z","iopub.status.idle":"2024-12-02T12:07:34.963215Z","shell.execute_reply.started":"2024-12-02T12:07:34.953236Z","shell.execute_reply":"2024-12-02T12:07:34.962479Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"!wget https://github.com/ANTsX/ANTsPy/releases/download/v0.5.4/antspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:34.966570Z","iopub.execute_input":"2024-12-02T12:07:34.966815Z","iopub.status.idle":"2024-12-02T12:07:36.854798Z","shell.execute_reply.started":"2024-12-02T12:07:34.966792Z","shell.execute_reply":"2024-12-02T12:07:36.853666Z"}},"outputs":[{"name":"stdout","text":"--2024-12-02 12:07:35--  https://github.com/ANTsX/ANTsPy/releases/download/v0.5.4/antspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nResolving github.com (github.com)... 140.82.116.4\nConnecting to github.com (github.com)|140.82.116.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://objects.githubusercontent.com/github-production-release-asset-2e65be/101671766/8d3c7d1d-41b3-451f-9791-5eb21f45a5ef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241202T120736Z&X-Amz-Expires=300&X-Amz-Signature=4a78e72549ec6b41236bab90a1ef7f420572afdd50670d7aa769d4577360e2be&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dantspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n--2024-12-02 12:07:36--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/101671766/8d3c7d1d-41b3-451f-9791-5eb21f45a5ef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241202T120736Z&X-Amz-Expires=300&X-Amz-Signature=4a78e72549ec6b41236bab90a1ef7f420572afdd50670d7aa769d4577360e2be&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dantspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&response-content-type=application%2Foctet-stream\nResolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 22202906 (21M) [application/octet-stream]\nSaving to: 'antspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'\n\nantspyx-0.5.4-cp310 100%[===================>]  21.17M  97.6MB/s    in 0.2s    \n\n2024-12-02 12:07:36 (97.6 MB/s) - 'antspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl' saved [22202906/22202906]\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"! pip install ./antspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:36.856061Z","iopub.execute_input":"2024-12-02T12:07:36.856386Z","iopub.status.idle":"2024-12-02T12:07:46.698464Z","shell.execute_reply.started":"2024-12-02T12:07:36.856349Z","shell.execute_reply":"2024-12-02T12:07:46.697475Z"}},"outputs":[{"name":"stdout","text":"Processing ./antspyx-0.5.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (2.2.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (6.0.2)\nRequirement already satisfied: numpy<=2.0.1 in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (1.26.4)\nRequirement already satisfied: statsmodels in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (0.14.2)\nRequirement already satisfied: webcolors in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (24.6.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (3.7.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (10.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from antspyx==0.5.4) (2.32.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->antspyx==0.5.4) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->antspyx==0.5.4) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->antspyx==0.5.4) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->antspyx==0.5.4) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->antspyx==0.5.4) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->antspyx==0.5.4) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->antspyx==0.5.4) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->antspyx==0.5.4) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->antspyx==0.5.4) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->antspyx==0.5.4) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->antspyx==0.5.4) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->antspyx==0.5.4) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->antspyx==0.5.4) (2024.8.30)\nRequirement already satisfied: scipy!=1.9.2,>=1.8 in /opt/conda/lib/python3.10/site-packages (from statsmodels->antspyx==0.5.4) (1.14.1)\nRequirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.10/site-packages (from statsmodels->antspyx==0.5.4) (0.5.6)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.6->statsmodels->antspyx==0.5.4) (1.16.0)\nInstalling collected packages: antspyx\nSuccessfully installed antspyx-0.5.4\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Initialize models and move to GPU\nvae_gan = model\n# mod = th.load(model_path, map_location='cuda')\n# vae_gan.load_state_dict(mod[\"model_state_dict\"])\n\ndef print_gpu_debug_info():\n    print(\"CUDA available:\", th.cuda.is_available())\n    print(\"Current device:\", th.cuda.current_device() if th.cuda.is_available() else \"CPU\")\n    print(\"Device count:\", th.cuda.device_count() if th.cuda.is_available() else 0)\n    print(\"Device name:\", th.cuda.get_device_name(0) if th.cuda.is_available() else \"CPU\")\n    \n    # Add memory usage info\n    if th.cuda.is_available():\n        print(\"\\nGPU Memory Usage:\")\n        print(f\"Allocated: {th.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n        print(f\"Cached: {th.cuda.memory_reserved(0) / 1024**2:.2f} MB\")","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:46.699925Z","iopub.execute_input":"2024-12-02T12:07:46.700249Z","iopub.status.idle":"2024-12-02T12:07:46.706570Z","shell.execute_reply.started":"2024-12-02T12:07:46.700218Z","shell.execute_reply":"2024-12-02T12:07:46.705684Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from torch.utils.data import Sampler\nfrom collections import defaultdict\nimport numpy as np\n\nclass ContrastiveBatchSampler(Sampler):\n    def __init__(self, labels):\n        self.labels = labels\n        self.label_to_indices = defaultdict(list)\n        for idx, label in enumerate(self.labels):\n            self.label_to_indices[label].append(idx)\n        self.labels_set = list(self.label_to_indices.keys())\n        self.num_samples = len(self.labels)\n        self.batch_size = 4  # Each batch will have 4 samples (two pairs)\n\n    def __iter__(self):\n        # Compute the number of batches\n        num_batches = len(self) \n        # Shuffle the class labels\n        np.random.shuffle(self.labels_set)\n        # Start generating batches\n        for _ in range(num_batches):\n            # Randomly select two classes\n            if len(self.labels_set) < 2:\n                # Re-shuffle if not enough classes\n                np.random.shuffle(self.labels_set)\n            classes = np.random.choice(self.labels_set, size=2, replace=False)\n            batch_indices = []\n            for cls in classes:\n                # Randomly select two indices from this class\n                indices = self.label_to_indices[cls]\n                if len(indices) >= 2:\n                    selected_indices = np.random.choice(indices, size=2, replace=False)\n                else:\n                    # If not enough samples, replicate indices\n                    selected_indices = np.random.choice(indices, size=2, replace=True)\n                batch_indices.extend(selected_indices)\n            yield batch_indices\n\n    def __len__(self):\n        # Calculate the number of batches\n        return self.num_samples // self.batch_size\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:46.707718Z","iopub.execute_input":"2024-12-02T12:07:46.707979Z","iopub.status.idle":"2024-12-02T12:07:46.721716Z","shell.execute_reply.started":"2024-12-02T12:07:46.707954Z","shell.execute_reply":"2024-12-02T12:07:46.720971Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from nilearn.datasets import load_mni152_template\nimport ants\n# Load the MNI template (default resolution: 2mm)\nmni_template = load_mni152_template()\ntemplate_path = mni_template.get_filename()\ntemplate_img_ants = ants.from_numpy(mni_template.get_fdata())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:46.722677Z","iopub.execute_input":"2024-12-02T12:07:46.722940Z","iopub.status.idle":"2024-12-02T12:07:48.344259Z","shell.execute_reply.started":"2024-12-02T12:07:46.722916Z","shell.execute_reply":"2024-12-02T12:07:48.343195Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"header_path = \"/kaggle/input/ds-sub2-download/ds001761-download/\"\nfrom torch.utils.data import Dataset\n\nclass FmriDataSet(Dataset):\n    def __init__(self, event_set, transform=None):\n        self.event_set = event_set\n        self.transform = transform\n        self.attribute_list = attribute_list\n        self.labels = event_set['class'].values  # Ensure 'class_label' is the correct column name\n\n    def __len__(self):\n        return len(self.event_set)\n\n    def __getitem__(self, idx):\n        row = self.event_set.iloc[idx]\n        subject_num = row[\"subject_number\"]\n        session_num = row[\"session_number\"]\n        run_num = row[\"run_number\"]\n        img_path = row[\"stim_file\"]\n        fmri_map = row[\"fmri_map\"].astype(int)\n        #attribute_values = row[self.attribute_list].fillna(0).astype(int).values\n        #attribute_set = th.tensor(attribute_values)#\n        gender = th.tensor(row[\"class\"])\n        if self.transform:\n            img = self.transform(img_path)\n        fmri_path = f\"/kaggle/input/ds-sub2-download/ds001761-download/sub-0{subject_num}/ses-0{session_num}/func/sub-0{subject_num}_ses-0{session_num}_task-faces_run-0{run_num}_bold.nii.gz\"\n        anat_path = f\"/kaggle/input/ds-sub2-download/ds001761-download/sub-0{subject_num}/ses-0{session_num}/anat/sub-0{subject_num}_ses-0{session_num}_acq-01_T1w.nii.gz\"\n        run_fmri = nib.load(fmri_path).get_fdata()[:, :, :, fmri_map]\n        #anat_img = nib.load(anat_path).get_fdata()\n        ## takes too long for now.\n        # Normalize the image\n        # fmri_img_ants = ants.from_numpy(run_fmri)\n        # anat_img_ants = ants.from_numpy(anat_img)\n        # anat2mni_transform = ants.registration(\n        #     fixed=template_img_ants,\n        #     moving=anat_img_ants,\n        #     type_of_transform='SyN',\n        # )\n        # fmri2mni = ants.apply_transforms(\n        #     fixed=template_img_ants,\n        #     moving=fmri_img_ants,\n        #     transformlist=anat2mni_transform['fwdtransforms']\n        # )\n        #fmri_output = th.from_numpy(fmri2mni.numpy())\n        fmri_output = th.from_numpy(run_fmri)\n        fmri_output = fmri_output.unsqueeze(0)\n        return img, run_fmri, gender\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:48.345737Z","iopub.execute_input":"2024-12-02T12:07:48.346962Z","iopub.status.idle":"2024-12-02T12:07:48.355960Z","shell.execute_reply.started":"2024-12-02T12:07:48.346922Z","shell.execute_reply":"2024-12-02T12:07:48.354999Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# header_path = \"/kaggle/input/ds-sub2-download/ds001761-download/\"\n# from torch.utils.data import Dataset\n# class FmriDataSet(Dataset):\n#     def __init__(self,event_set,transform=None):\n#         self.event_set = event_set\n#         self.transform=transform\n#         self.attribute_list = attribute_list\n#     def __len__(self):\n#         return(len(self.event_set))\n#     def __getitem__(self,idx):\n#         row = self.event_set.iloc[idx]\n#         subject_num = row[\"subject_number\"]\n#         session_num = row[\"session_number\"]\n#         run_num = row[\"run_number\"]\n#         img_path = row[\"stim_file\"]\n#         fmri_map = row[\"fmri_map\"].astype(int)\n#         attribute_values = row[self.attribute_list].fillna(0).astype(int).values\n#         attribute_set = th.tensor(attribute_values)\n#         if self.transform:\n#             img = self.transform(img_path)\n#         fmri_path = f\"/kaggle/input/ds-sub2-download/ds001761-download/sub-0{subject_num}/ses-0{session_num}/func/sub-0{subject_num}_ses-0{session_num}_task-faces_run-0{run_num}_bold.nii.gz\"\n#         anat_path = f\"/kaggle/input/ds-sub2-download/ds001761-download/sub-0{subject_num}/ses-0{session_num}/anat/sub-0{subject_num}_ses-0{session_num}_acq-01_T1w.nii.gz\"\n#         run_fmri  = nib.load(fmri_path).get_fdata()[:,:,:,fmri_map]\n#         anat_img  =nib.load(anat_path)\n#         ## now im going to normalize said image\n#         antspy_fmri = antspy.from_numpy(run_fmri)\n        \n#         anat2mni_transform = ants.registration(\n#             fixed=template_img_ants,\n#             moving=anat_img_ants,\n#             type_of_transform='SyN',\n#         )\n#         fmri2mni = ants.apply_transforms(\n#             fixed=template_img_ants,\n#             moving=fmri_img_ants,\n#             transformlist=anat2mni_transform['fwdtransforms']\n#         )\n\n#         fmri_output = th.from_numpy(fmri2mni)      \n#         fmri_output = fmri_output.unsqueeze(0)\n#         return(img,run_fmri,attribute_set)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:48.357354Z","iopub.execute_input":"2024-12-02T12:07:48.357733Z","iopub.status.idle":"2024-12-02T12:07:48.374309Z","shell.execute_reply.started":"2024-12-02T12:07:48.357696Z","shell.execute_reply":"2024-12-02T12:07:48.373487Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_events,test_events = train_test_split(event_set,test_size=0.2)\ntrain_events,val_events = train_test_split(event_set,test_size=0.2)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:48.375532Z","iopub.execute_input":"2024-12-02T12:07:48.375853Z","iopub.status.idle":"2024-12-02T12:07:48.397736Z","shell.execute_reply.started":"2024-12-02T12:07:48.375818Z","shell.execute_reply":"2024-12-02T12:07:48.396711Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"train_fmri_dataset = FmriDataSet(train_events,load_and_transform_image)\ntest_fmri_dataset =  FmriDataSet(test_events,load_and_transform_image)\nval_fmri_dataset = FmriDataSet(val_events,load_and_transform_image)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:48.398865Z","iopub.execute_input":"2024-12-02T12:07:48.399214Z","iopub.status.idle":"2024-12-02T12:07:48.403923Z","shell.execute_reply.started":"2024-12-02T12:07:48.399177Z","shell.execute_reply":"2024-12-02T12:07:48.403204Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"batch_sampler = ContrastiveBatchSampler(train_fmri_dataset.labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:48.404828Z","iopub.execute_input":"2024-12-02T12:07:48.405062Z","iopub.status.idle":"2024-12-02T12:07:48.417999Z","shell.execute_reply.started":"2024-12-02T12:07:48.405039Z","shell.execute_reply":"2024-12-02T12:07:48.417267Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_dataloader = DataLoader(train_fmri_dataset, batch_sampler=batch_sampler)\ntest_dataloader = DataLoader(test_fmri_dataset, batch_size=4)\nval_dataloader = DataLoader(test_fmri_dataset, batch_size=8)","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:48.420685Z","iopub.execute_input":"2024-12-02T12:07:48.420933Z","iopub.status.idle":"2024-12-02T12:07:48.429020Z","shell.execute_reply.started":"2024-12-02T12:07:48.420910Z","shell.execute_reply":"2024-12-02T12:07:48.428227Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# progress_bar = tqdm(enumerate(test_dataloader), desc=f\"Epoch {epoch+1}\", total=len(test_dataloader), leave=False)\n\nfor i, data in enumerate(train_dataloader):\n    img_batch, fmri_data_batch,attribute_set = data\n    print(img_batch.shape, fmri_data_batch.shape)\n    print(attribute_set,attribute_set.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:48.429977Z","iopub.execute_input":"2024-12-02T12:07:48.430231Z","iopub.status.idle":"2024-12-02T12:07:54.747147Z","shell.execute_reply.started":"2024-12-02T12:07:48.430206Z","shell.execute_reply":"2024-12-02T12:07:54.746196Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 3, 150, 150]) torch.Size([4, 80, 80, 41])\ntensor([-1, -1,  1,  1]) torch.Size([4])\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!mkdir /kaggle/working/models/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:54.748157Z","iopub.execute_input":"2024-12-02T12:07:54.748475Z","iopub.status.idle":"2024-12-02T12:07:55.829123Z","shell.execute_reply.started":"2024-12-02T12:07:54.748414Z","shell.execute_reply":"2024-12-02T12:07:55.827887Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"loss_criterion= nn.CrossEntropyLoss()\ndef pair_contrastive_loss(e1,e2,c1,c2):\n    # fmri_embeddings will be a (2,fmri_shape) vector , the classes vector correpond to both\n    loss = (c1 == c2)*(loss_criterion(e1,e2)) + (c1!=c2)*(max(0,loss_criterion(e1,e2)))\n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:55.830745Z","iopub.execute_input":"2024-12-02T12:07:55.831043Z","iopub.status.idle":"2024-12-02T12:07:55.836722Z","shell.execute_reply.started":"2024-12-02T12:07:55.831014Z","shell.execute_reply":"2024-12-02T12:07:55.835633Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"num_epochs = 3\noptimizer = th.optim.Adam(brain2latent.parameters(), lr=1e-3)\n\nfor epoch in range(num_epochs):\n    construction_loss_value = 0\n    epoch_loss = 0\n    progress_bar = tqdm(enumerate(train_dataloader), desc=f\"Epoch {epoch+1}\", total=len(train_dataloader), leave=False)\n\n    for i, data in progress_bar:\n        img_batch, fmri_data_batch,classes = data\n        img_batch = img_batch.view(-1, 3, IMAGE_SIZE, IMAGE_SIZE)  # Reshape if needed\n\n        # Forward pass\n        with th.no_grad():\n            encoded_mu, encoded_logvar = vae_gan.encode(img_batch.to(device, non_blocking=True))\n\n        fmri_data_batch = fmri_data_batch.reshape(4,1,80,80,41)\n        #print(fmri_data_batch.shape)\n        latent_mu, latent_logvar = brain2latent(fmri_data_batch.float().to(device, non_blocking=True))\n        \n        # Compute MSE loss\n        construction_loss = F.mse_loss(encoded_mu, latent_mu)\n        construction_loss += F.mse_loss(encoded_logvar, latent_logvar)\n        construction_loss_value = construction_loss.item()\n        for i in range(4):\n            #print(encoded_mu[i].shape)\n            for j in range(i+1,4):\n                construction_loss += pair_contrastive_loss(encoded_mu[i],encoded_mu[j],classes[i],classes[j] ) \n\n        \n        # Backward pass\n        optimizer.zero_grad()\n        construction_loss.backward()\n        optimizer.step()\n\n        # Update loss\n        epoch_loss += construction_loss.item()\n        \n        if (i % 100 == 0) and i != 0:\n            print_gpu_debug_info()\n            \n        progress_bar.set_description(f\"Epoch {epoch+1}, TrainLoss: {construction_loss_value:.4f}\")\n        th.cuda.empty_cache()\n        gc.collect()\n\n    save_path = f'/kaggle/working/models/brain2latent_epoch{epochs+1}.pth'\n    th.save({\n        'epoch': num_epochs,\n        'model_state_dict': brain2latent.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': epoch_loss,\n    }, save_path)\n    \n    \n    print(f\"Epoch {epoch+1} completed, Average Loss: {epoch_loss / len(train_dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T12:07:55.837897Z","iopub.execute_input":"2024-12-02T12:07:55.838151Z"}},"outputs":[{"name":"stderr","text":"Epoch 1, TrainLoss: 1.1037:  38%|███▊      | 922/2457 [1:09:32<1:54:17,  4.47s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Assuming IMAGE_SIZE is defined elsewhere\nIMAGE_SIZE = 150\n\nbrain2latent.eval()\nvae_gan.eval()\navg_test_loss = 0\nepoch = 0\ncount = 0\n\nprogress_bar = tqdm(enumerate(test_dataloader), desc=f\"Epoch {epoch+1}\", total=len(test_dataloader), leave=False)\n\nfor i, data in progress_bar:\n    img_batch, fmri_data_batch = data\n    test_loss = 0\n\n    # Forward pass\n    with th.no_grad():\n        # Encode fMRI data and images\n        encoded_mu, encoded_logvar = vae_gan.encode(img_batch.to(device, non_blocking=True))\n        latent_mu, latent_logvar = brain2latent(fmri_data_batch.float().to(device, non_blocking=True))\n        print(encoded_mu.shape,encoded_logvar.shape)\n        # Compute MSE loss\n        test_loss = F.mse_loss(encoded_mu, latent_mu)\n        test_loss += F.mse_loss(encoded_logvar, latent_logvar)\n        test_lossvalue = test_loss.item()\n        print(f\"Test Loss: {test_lossvalue:.4f}\")\n        \n        avg_test_loss += test_loss\n\n        # Decode generated images from latent space\n        img = vae_gan.decode(vae_gan.reparameterize(latent_mu, latent_logvar))\n        \n        # Generate the reconstruction of the original image batch\n        vaegan_imgs = vae_gan.decode(vae_gan.reparameterize(encoded_mu, encoded_logvar))\n\n        # Convert tensors to CPU numpy arrays for visualization\n        img = img.cpu().detach().numpy()\n        print(vaegan_imgs)\n        vaegan_imgs = vaegan_imgs.cpu().detach().numpy()\n        img_batch = img_batch.cpu().detach().numpy()\n\n        # Plot original, reconstructed, and generated images\n        fig, axes = plt.subplots(4, 3, figsize=(12, 10))  # 4 rows for 4 images, 3 columns\n        for j in range(4):  # Plot first 4 images in the batch\n            # Original Image\n            axes[j, 0].imshow(np.clip(img_batch[j].transpose(1, 2, 0), 0, 1))\n            axes[j, 0].axis(\"off\")\n            axes[j, 0].set_title(\"Original Image\")\n\n            # Reconstructed Image\n            axes[j, 1].imshow(np.clip(vaegan_imgs[j].transpose(1, 2, 0), 0, 1))\n            axes[j, 1].axis(\"off\")\n            axes[j, 1].set_title(\"Reconstructed Image\")\n\n            # Generated Image\n            axes[j, 2].imshow(np.clip(img[j].transpose(1, 2, 0), 0, 1))\n            axes[j, 2].axis(\"off\")\n            axes[j, 2].set_title(\"Generated Image\")\n\n        plt.tight_layout()\n        plt.show()\n\n    break  # Process only the first batch for visualization\n    count += 1\n    if count == 10:  # Limit to 10 iterations\n        break\n\n    progress_bar.set_description(f\"Epoch {epoch+1}, TestLoss: {test_lossvalue:.4f}\")\n\n# Compute the average test loss\navg_test_loss = avg_test_loss / (i + 1)\nprint(f\"Average Test Loss: {avg_test_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/vae-torch-celeba/ds001761-download/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save at the end of training\nmodel_save_path = 'brain2latent_final.pth'\nth.save({\n    'epoch': num_epochs,\n    'model_state_dict': brain2latent.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': epoch_loss,\n}, model_save_path)\n\n\n\n# To load the model later:\ndef load_model(path):\n    checkpoint = th.load(path)\n    brain2latent.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch'], checkpoint['loss']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('./brain2latent_final.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('zipped_file_name', 'zip', '/vae-torch-celeba/brain2latent_final.pth')\nfrom IPython.display import FileLink\nFileLink(r'zipped_file_name.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save at the end of training\nmodel_save_path = 'brain2latent_final3.pth'\nth.save({\n    'model_state_dict': brain2latent.state_dict(),\n}, model_save_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}